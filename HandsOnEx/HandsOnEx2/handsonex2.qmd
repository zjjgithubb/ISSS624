---
title: "Hands-On Ex 2"

date: "23 Nov 23"
date-modified: "last-modified" 

format: html
execute: 
  echo: true
  eval: true
  warning: false
  
editor: visual 
---

# Overview

In this hands-on exercise, we will learn how to compute spatial weights using R. By the end to this hands-on exercise, we will be able to:

-   import geospatial data using appropriate function(s) of **sf** package,

-   import csv file using appropriate function of **readr** package,

-   perform relational join using appropriate join function of **dplyr** package,

-   compute spatial weights using appropriate functions of **spdep** package, and

-   calculate spatially lagged variables using appropriate functions of **spdep** package

In the second part -

We will learn how to compute Global and Local Measure of Spatial Autocorrelation (GLSA) by using **spdep** package. By the end to this hands-on exercise, we will be able to:

-   compute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of **spdep** package,

    -   plot Moran scatterplot,

    -   compute and plot spatial correlogram using appropriate function of **spdep** package.

-   compute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions **spdep** package;

-   compute Getis-Ord's Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of **spdep** package; and

-   to visualise the analysis output by using **tmap** package.

## The Study Area and Data

Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is **No**. Then, our next question will be "is there sign of spatial clustering?". And, if the answer for this question is yes, then our next question will be "where are these clusters?"

We will be using the following data sets:

-   Geospatial Data: Hunan County Boundary Layer

-   Aspatial Data: Hunan's local development indicators in 2012. Hunan_2012.csv

## Loading the Packages

We will be using the following packages for this exercise:

New package for this exercise - **spdep**

The **spdep package** is a collection of functions for spatial data analysis in R. It provides tools for creating spatial weights matrices, performing spatial autocorrelation tests, and fitting spatial regression models. The package is widely used in a variety of fields, including geography, economics, and ecology.

```{r}

pacman::p_load(sf, spdep, tmap, tidyverse, knitr)
```

# Getting the data ready

First - we will import the data.

We will start with the geospatial data - Hunan county, which is in ESRI shapefile format.

```{r}

hunan <- st_read(dsn = "data/geospatial", 
                 layer = "Hunan")
```

Next, we will import the aspatial data using the readr package.

```{r}

hunan2012 <- read_csv("data/aspatial/Hunan_2012.csv")
```

Quick check on the data to learn more about the data. It lists the County name and the various indicators.

```{r}

head(hunan2012)
```

## Performing relational join

We will update the attribute table update the attribute table of *hunan*'s SpatialPolygonsDataFrame with the attribute fields of *hunan2012* dataframe. This is performed by using *left_join()* of **dplyr** package.

```{r}

hunan <- left_join(hunan,hunan2012)%>%
  select(1:4, 7, 15)
```

1.  **`left_join(hunan, hunan2012)`**: This line performs a left join between the data frames **`hunan`** and **`hunan2012`**. The left join includes all rows from the **`hunan`** data frame and the matching rows from the **`hunan2012`** data frame based on a common key or keys. The result is a data frame with columns from both data frames.

2.  **`%>%`**: The pipe operator (**`%>%`**) is used to pass the result of the left join to the next operation.

3.  **`select(1:4, 7, 15)`**: This line selects specific columns from the joined data frame. Columns 1 through 4, column 7, and column 15 are retained in the final data frame, and the rest are dropped.

## Visualising the Data - Regional Development Indicator

We will prepare a base map and a choropleth map showing the distribution of GDPPC 2012 by using *qtm()* of **tmap** package.

```{r}

basemap <- tm_shape(hunan) +
  tm_polygons() +
  tm_text("NAME_3", size=0.5)

gdppc <- qtm(hunan, "GDPPC")
tmap_arrange(basemap, gdppc, asp=1, ncol=2)
```

```{r}
equal <- tm_shape(hunan) +
  tm_fill("GDPPC",
          n = 5,
          style = "equal") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal interval classification")

quantile <- tm_shape(hunan) +
  tm_fill("GDPPC",
          n = 5,
          style = "quantile") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal quantile classification")

tmap_arrange(equal, 
             quantile, 
             asp=1, 
             ncol=2)
```

# Computing Contiguity Spatial Weights

## Overview of Contiguity Spatial Weights

In spatial analysis, contiguity spatial weights represent the connections or relationships between spatial units, such as regions, countries, or points on a map. These weights are typically constructed based on the geometric proximity of the units, indicating whether they share a common border or are within a certain distance of each other. The specific type of contiguity used depends on the research question and the nature of the data.

**Types of Contiguity Spatial Weights**

There are two main types of contiguity spatial weights:

1.  **Rook contiguity:** Units are considered neighbors if they share a common edge.

2.  **Queen contiguity:** Units are considered neighbors if they share a common edge or vertex.

These two types of contiguity differ in how they define connectivity. Rook contiguity is more restrictive, while queen contiguity is more inclusive. The choice of contiguity weight depends on the specific application and the research question.

**Applications of Contiguity Spatial Weights**

Contiguity spatial weights are used in various spatial analysis applications, including:

1.  **Spatial autocorrelation analysis:** Identifying patterns of similarity or dissimilarity in spatial data.

2.  **Spatial regression modeling:** Accounting for spatial dependence in statistical models.

3.  **Spatial interpolation:** Estimating values at unsampled locations based on surrounding values.

4.  **Spatial diffusion modeling:** Analyzing the spread of phenomena over space and time.

**Construction of Contiguity Spatial Weights**

Contiguity spatial weights are typically constructed using spatial data structures and algorithms. These tools allow for efficient and accurate identification of neighbors based on geometric proximity. The specific construction method depends on the type of spatial data (polygons, points, lines) and the type of contiguity (rook or queen).

**Normalization of Contiguity Spatial Weights**

Contiguity spatial weights are often normalized to ensure that they are comparable across different spatial units and studies. Normalization methods can involve row-standardization, column-standardization, or other techniques.

**Importance of Contiguity Spatial Weights**

Contiguity spatial weights play a crucial role in spatial analysis by providing a formal representation of spatial relationships between units. They allow researchers to incorporate spatial dependence into statistical models and analyze the impact of spatial context on various phenomena.

## Compute QUEEN contiguity based neighbours

Computing the Queen contiguity weight matrix.

```{r}

wm_q <- poly2nb(hunan, queen=TRUE)
summary(wm_q)
```

The summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.

For each polygon in our polygon object, *wm_q* lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:

```{r}

wm_q[[1]]
```

Polygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.

We can retrive the county name of Polygon ID=1 by using the code chunk below:

```{r}

hunan$County[1]
```

The output reveals that Polygon ID=1 is Anxiang county.

To reveal the county names of the five neighboring polygons, the code chunk will be used:

```{r}

hunan$NAME_3[c(2,3,4,57,85)]
```

We can retrieve the GDPPC of these five countries by using the code chunk below.

```{r}

nb1 <- wm_q[[1]]
nb1 <- hunan$GDPPC[nb1]
nb1
```

The printed output above shows that the GDPPC of the five nearest neighbours based on Queen's method are 20981, 34592, 24473, 21311 and 22879 respectively.

We can display the complete weight matrix by using *str()*.

```{r}
str(wm_q)
```

## Computing ROOK contiguity based neighbours

The code chunk below is used to compute Rook contiguity weight matrix.

```{r}
wm_r <- poly2nb(hunan, queen=FALSE)
summary(wm_r)
```

The summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one neighbours.

## Visualising contiguity weights

A connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs. Getting Latitude and Longitude of Polygon Centroids

We will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid. We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation

To get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation \[\[\]\] and 1. This allows us to get only the longitude, which is the first value in each centroid.

```{r}
longitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])
```

We do the same for latitude with one key difference. We access the second value per each centroid with \[\[2\]\].

```{r}
latitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])
```

Now that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.

```{r}
coords <- cbind(longitude, latitude)
```

We check to see if things are formatted correctly.

```{r}
head(coords)
```

### Plotting QUEEN map

```{r}
plot(hunan$geometry, border="lightgrey")
plot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= "red")
```

### Plotting ROOK map

```{r}
plot(hunan$geometry, border="lightgrey")
plot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = "red")
```

### Plotting both QUEEN and ROOK maps

```{r}
par(mfrow=c(1,2))
plot(hunan$geometry, border="lightgrey", main="Queen Contiguity")
plot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= "red")
plot(hunan$geometry, border="lightgrey", main="Rook Contiguity")
plot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = "red")
```

## Computing distance based neighbours

In this section, you will learn how to derive distance-based weight matrices by using [*dnearneigh()*](https://r-spatial.github.io/spdep/reference/dnearneigh.html) of **spdep** package.

The function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in **km** will be calculated assuming the WGS84 reference ellipsoid.

### Determine the cut-off distance

Firstly, we need to determine the upper limit for distance band by using the steps below:

-   Return a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using [*knearneigh()*](https://r-spatial.github.io/spdep/reference/knearneigh.html) of **spdep**.

-   Convert the knn object returned by *knearneigh()* into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using [*knn2nb()*](https://r-spatial.github.io/spdep/reference/knn2nb.html).

-   Return the length of neighbour relationship edges by using [*nbdists()*](https://r-spatial.github.io/spdep/reference/nbdists.html) of **spdep**. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.

-   Remove the list structure of the returned object by using [**unlist()**](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/unlist).

```{r}
#coords <- coordinates(hunan)
k1 <- knn2nb(knearneigh(coords))
k1dists <- unlist(nbdists(k1, coords, longlat = TRUE))
summary(k1dists)
```

The summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.

### Computing fixed distance weight matrix

Now, we will compute the distance weight matrix by using *dnearneigh()* as shown in the code chunk below.

dnearneigh(your_spatial_data, d1 = distance_threshold, longlat = FALSE) - **`dnearneigh`**: This function from the **`spdep`** package creates a neighbors list based on a specified distance threshold (**`d1`**). The **`longlat`** parameter is set to **`FALSE`** because it assumes the data is in planar coordinates. But we are using longlat so it is set to TRUE.

```{r}
wm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)
wm_d62
```

-   **Number of regions:** There are 88 regions in your spatial data.

-   **Number of nonzero links:** The total number of links or connections between regions is 324.

-   **Percentage nonzero weights:** This is the percentage of nonzero weights in the spatial weights matrix. In your case, it's 4.183884%, indicating that a small percentage of all possible pairwise connections have been defined as neighbors.

-   **Average number of links:** On average, each region has approximately 3.68 links or neighbors.

Next, we will use *str()* to display the content of wm_d62 weight matrix.

```{r}
str(wm_d62)
```

Another way to display the structure of the weight matrix is to combine [*table()*](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/table) and [*card()*](https://r-spatial.github.io/spdep/reference/card.html) of spdep.

```{r}
table(hunan$County, card(wm_d62))
```

```{r}
n_comp <- n.comp.nb(wm_d62)
n_comp$nc
```

```{r}
table(n_comp$comp.id)
```

### Plotting fixed distance weight matrix

Next - we will plot the data onto the map.

```{r}
plot(hunan$geometry, border="lightgrey")
plot(wm_d62, coords, add=TRUE)
plot(k1, coords, add=TRUE, col="red", length=0.08)
```

The red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.

Alternatively, we can plot both of them next to each other by using the code chunk below.

```{r}
par(mfrow=c(1,2))
plot(hunan$geometry, border="lightgrey", main="1st nearest neighbours")
plot(k1, coords, add=TRUE, col="red", length=0.08)
plot(hunan$geometry, border="lightgrey", main="Distance link")
plot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)
```

## Computing adaptive distance weight matrix

One of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.

It is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.

-   **`knn2nb`**: This function converts a k-nearest neighbors list to a neighbor list object. It takes the output of the **`knearneigh`** function (which calculates k-nearest neighbors) and converts it to a neighbor list suitable for spatial analysis.

-   **`knearneigh(coords, k=6)`**: This function calculates the k-nearest neighbors for a given set of coordinates (**`coords`**) with a specified value of **`k`** (in this case, **`k = 6`**).

```{r}
knn6 <- knn2nb(knearneigh(coords, k=6))
knn6
```

Similarly, we can display the content of the matrix by using *str()*.

```{r}
str(knn6)
```

### Plotting distance based neighbours

```{r}
plot(hunan$geometry, border="lightgrey")
plot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = "red")
```

## Weights based on Inverse Distance Weighting

The IDW method estimates values for unmeasured locations based on the values observed at nearby locations. The basic idea is that values at a particular location are influenced more by values at nearby locations than by those farther away.

Here's a brief explanation of how IDW works:

1.  **Inverse Distance Weighting:**

    -   IDW assigns weights to measured values at known locations based on their distance to the location where you want to estimate a value.

    -   The weights are typically inversely proportional to the distance from the known points. Closer points have higher weights, indicating a stronger influence on the estimated value.

2.  **Weighted Average:**

    -   The estimated value at a specific location is calculated as a weighted average of the observed values at nearby locations, with weights determined by their inverse distances.

3.  **Power Parameter:**

    -   IDW often includes a power parameter (p) that allows you to control the rate at which the weights decrease with distance. A higher value of p means that closer points have even more influence on the estimated value.

First, we will compute the distances between areas by using [*nbdists()*](https://r-spatial.github.io/spdep/reference/nbdists.html) of **spdep**.

```{r}
dist <- nbdists(wm_q, coords, longlat = TRUE)
ids <- lapply(dist, function(x) 1/(x))
ids
```

### Row-standardised weights matrix

Next, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style="W"). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors' values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data.

For this example, we'll stick with the style="W" option for simplicity's sake but note that other more robust options are available, notably style="B".

```{r}
rswm_q <- nb2listw(wm_q, style="W", zero.policy = TRUE)
rswm_q
```

The input of *nb2listw()* must be an object of class **nb**. The syntax of the function has two major arguments, namely style and zero.poly.

-   *style* can take values "W", "B", "C", "U", "minmax" and "S". B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).

-   If *zero policy* is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %\*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.

-   The zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.

To see the weight of the first polygon's eight neighbors type:

```{r}
rswm_q$weights[10]
```

Each neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor's income will be multiplied by 0.2 before being tallied.

Using the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.

```{r}
rswm_ids <- nb2listw(wm_q, glist=ids, style="B", zero.policy=TRUE)
rswm_ids
```

```{r}
rswm_ids$weights[1]
```

```{r}
summary(unlist(rswm_ids$weights))
```

## Application of Spatial Weight Matrix

1.  **Spatial Lag with Row-Standardized Weights:**

    -   This involves creating a new variable for each observation that represents the average or weighted average of the variable of interest in its neighboring locations.

    -   The weights used for the spatial lag are row-standardized weights, meaning that each row of the spatial weights matrix sums to 1.

2.  **Spatial Lag as a Sum of Neighboring Values:**

    -   Instead of using row-standardized weights, this approach calculates the spatial lag as the sum of the values of the variable of interest in neighboring locations.

    -   This is a simple summation of the neighboring values without normalization.

3.  **Spatial Window Average:**

    -   This involves creating a new variable for each observation that represents the average value of the variable of interest within a specified spatial window or neighborhood around that observation.

    -   The window can be defined by distance, number of neighbors, or some other criterion.

4.  **Spatial Window Sum:**

    -   Similar to the spatial window average, this creates a new variable that represents the sum of the variable of interest within a specified spatial window or neighborhood around each observation.

The choice of which spatial lag variable to use depends on the specific research question, the characteristics of your data, and the underlying assumptions of your analysis. Here's a general guide on when to use each of the four spatial lag variables you mentioned:

1.  **Spatial Lag with Row-Standardized Weights:**
    -   **Use Case:** This is a commonly used spatial lag variable when you want to account for spatial autocorrelation and consider the average or weighted average of a variable in neighboring locations.
    -   **Considerations:** Useful when you want each observation's influence to be normalized by its connectivity to neighbors. It is often used in spatial econometrics.
2.  **Spatial Lag as a Sum of Neighboring Values:**
    -   **Use Case:** When you are interested in capturing the cumulative impact of neighboring values on a variable, without normalizing by the number of neighbors.
    -   **Considerations:** May be appropriate when you want to emphasize the total influence of neighbors, regardless of the number of neighbors.
3.  **Spatial Window Average:**
    -   **Use Case:** Useful when you want to smooth the variable by considering the average value within a specified spatial window or neighborhood around each observation.
    -   **Considerations:** Good for capturing a local spatial pattern while reducing noise. The size of the window affects the level of smoothing.
4.  **Spatial Window Sum:**
    -   **Use Case:** When you want to emphasize the cumulative impact of the variable within a specified spatial window or neighborhood.
    -   **Considerations:** Similar to the spatial window average, but places more weight on higher values within the window. Useful when you want to capture intensity or density rather than an average.

Consider the following factors when choosing which spatial lag variable to use:

-   **Spatial Patterns:** Consider the spatial patterns in your data. Are you interested in capturing local averages, totals, or smoothing effects?

-   **Assumptions:** Think about the assumptions underlying each approach. For example, row-standardized weights normalize the influence of neighbors, while a simple sum does not.

-   **Interpretability:** Choose the variable that aligns with the interpretability of your research question. What type of spatial influence are you trying to capture?

-   **Modeling Goals:** If you're incorporating these spatial lag variables into a regression model, consider the assumptions and requirements of the modeling technique you are using.

Experimenting with different spatial lag variables and assessing their impact on your analysis can also provide insights into which approach is most appropriate for your specific context.

### Spatial lag with row-standardised weights

Finally, we'll compute the average neighbor GDPPC value for each polygon. These values are often referred to as **spatially lagged values**.

```{r}
GDPPC.lag <- lag.listw(rswm_q, hunan$GDPPC)
GDPPC.lag
```

We utilised the rswm_q here.

Recalled in the previous section, we retrieved the GDPPC of these five countries by using the code chunk below.

```{r}
nb1 <- wm_q[[1]]
nb1 <- hunan$GDPPC[nb1]
nb1
```

Question: Can you see the meaning of Spatial lag with row-standardized weights now? - We are summarising the values of hte variable in the neighbouring locations in a way that fives equal importance to each neighbour and ensured that the influence of neghbours is normalised.

We can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.

```{r}
lag.list <- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))
lag.res <- as.data.frame(lag.list)
colnames(lag.res) <- c("NAME_3", "lag GDPPC")
hunan <- left_join(hunan,lag.res)
```

The following table shows the average neighboring income values (stored in the Inc.lag object) for each county.

```{r}
head(hunan)
```

Next, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.

```{r}
gdppc <- qtm(hunan, "GDPPC")
lag_gdppc <- qtm(hunan, "lag GDPPC")
tmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)
```

### Spatial lag as a sum of neighbouring values

We can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.

We start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.

```{r}
b_weights <- lapply(wm_q, function(x) 0*x + 1)
b_weights2 <- nb2listw(wm_q, 
                       glist = b_weights, 
                       style = "B")
b_weights2
```

With the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.

```{r}
lag_sum <- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))
lag.res <- as.data.frame(lag_sum)
colnames(lag.res) <- c("NAME_3", "lag_sum GDPPC")
```

Examine the result.

```{r}
lag_sum
```

Question: Can you understand the meaning of Spatial lag as a sum of neighboring values now? a simple summation of the neighbouring values without normalisation.

Next, we will append the *lag_sum GDPPC* field into `hunan` sf data frame by using the code chunk below.

```{r}
hunan <- left_join(hunan, lag.res)
```

Now, We can plot both the *GDPPC* and *Spatial Lag Sum GDPPC* for comparison using the code chunk below.

```{r}
gdppc <- qtm(hunan, "GDPPC")
lag_sum_gdppc <- qtm(hunan, "lag_sum GDPPC")
tmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)
```

### Spatial window average

The spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.

To add the diagonal element to the neighbour list, we just need to use *include.self()* from **spdep**.

```{r}
wm_qs <- include.self(wm_q)
```

Notice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909

Let us take a good look at the neighbour list of area \[1\] by using the code chunk below.

```{r}
wm_qs[[1]]
```

Notice that now \[1\] has six neighbours instead of five.

Now we obtain weights with *nb2listw()*

```{r}
wm_qs <- nb2listw(wm_qs)
wm_qs

```

Again, we use *nb2listw()* and *glist()* to explicitly assign weight values.

Lastly, we just need to create the lag variable from our weight structure and GDPPC variable.

```{r}
lag_w_avg_gpdpc <- lag.listw(wm_qs, 
                             hunan$GDPPC)
lag_w_avg_gpdpc
```

Next, we will convert the lag variable listw object into a data.frame by using *as.data.frame()*.

```{r}
lag.list.wm_qs <- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))
lag_wm_qs.res <- as.data.frame(lag.list.wm_qs)
colnames(lag_wm_qs.res) <- c("NAME_3", "lag_window_avg GDPPC")
```

Note: The third command line on the code chunk above renames the field names of *lag_wm_q1.res* object into *NAME_3* and *lag_window_avg GDPPC* respectively.

Next, the code chunk below will be used to append *lag_window_avg GDPPC* values onto *hunan* sf data.frame by using *left_join()* of **dplyr** package.

```{r}
hunan <- left_join(hunan, lag_wm_qs.res)
```

To compare the values of lag GDPPC and Spatial window average, `kable()` of Knitr package is used to prepare a table using the code chunk below.

```{r}
hunan %>%
  select("County", 
         "lag GDPPC", 
         "lag_window_avg GDPPC") %>%
  kable()
```

Lastly, *qtm()* of **tmap** package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.

```{r}
w_avg_gdppc <- qtm(hunan, "lag_window_avg GDPPC")
tmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)
```

### Spatial window sum

The spatial window sum is the counter part of the window average, but without using row-standardized weights.

To add the diagonal element to the neighbour list, we just need to use *include.self()* from **spdep**.

```{r}
wm_qs <- include.self(wm_q)
wm_qs
```

Next, we will assign binary weights to the neighbour structure that includes the diagonal element.

```{r}
b_weights <- lapply(wm_qs, function(x) 0*x + 1)
b_weights[1]
```

Notice that now \[1\] has six neighbours instead of five.

Again, we use *nb2listw()* and *glist()* to explicitly assign weight values.

```{r}
b_weights2 <- nb2listw(wm_qs, 
                       glist = b_weights, 
                       style = "B")
b_weights2
```

With our new weight structure, we can compute the lag variable with *lag.listw()*.

```{r}
w_sum_gdppc <- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))
w_sum_gdppc
```

Next, we will convert the lag variable listw object into a data.frame by using *as.data.frame()*.

```{r}
w_sum_gdppc.res <- as.data.frame(w_sum_gdppc)
colnames(w_sum_gdppc.res) <- c("NAME_3", "w_sum GDPPC")
```

Note: The second command line on the code chunk above renames the field names of *w_sum_gdppc.res* object into *NAME_3* and *w_sum GDPPC* respectively.

Next, the code chunk below will be used to append *w_sum GDPPC* values onto *hunan* sf data.frame by using *left_join()* of **dplyr** package.

```{r}
hunan <- left_join(hunan, w_sum_gdppc.res)
```

To compare the values of lag GDPPC and Spatial window average, `kable()` of Knitr package is used to prepare a table using the code chunk below.

```{r}
hunan %>%
  select("County", "lag_sum GDPPC", "w_sum GDPPC") %>%
  kable()
```

Lastly, *qtm()* of **tmap** package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.

```{r}
w_sum_gdppc <- qtm(hunan, "w_sum GDPPC")
tmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)
```

# Global Measures of Spatial Autocorrelation

## Computing Contiguity Spatial Weights

Before we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.

These are the same weights that were utilised in the earlier sections.

Queen Contiguity Weight matrix:

```{r}
summary(wm_q)
```

Row-standardised Weight Matrix:

```{r}
rswm_q
```

## Global Spatial Autocorrelation: Moran's I

We will learn how to perform Moran's I statistics testing by using [*moran.test()*](https://r-spatial.github.io/spdep/reference/moran.test.html) of **spdep.**

Moran's test is a statistical test used in spatial statistics to assess the presence of spatial autocorrelation in a dataset. Spatial autocorrelation refers to the tendency of similar values to be clustered together in space. Moran's test is named after Patrick Alfred Pierce Moran, who introduced the concept.

The test is based on Moran's I statistic, which quantifies the degree of spatial autocorrelation. Moran's I ranges from -1 (indicating perfect dispersion) to 1 (indicating perfect positive spatial autocorrelation). A value close to 0 suggests a random spatial pattern.

Here's a brief overview of how Moran's test works:

1.  **Null Hypothesis (H0):** There is no spatial autocorrelation in the dataset (random spatial pattern).

2.  **Alternative Hypothesis (H1):** There is spatial autocorrelation in the dataset (values are clustered or dispersed in space).

3.  **Moran's I Statistic:**

    -   The Moran's I statistic is computed based on the observed values of a variable and their spatial locations.

    -   ![](Moran%20Test.png)

4.  **Expected Moran's I Under the Null Hypothesis:**

    -   The expected value of Moran's I under the null hypothesis is computed based on a random spatial pattern (no autocorrelation).

5.  **Test Statistic:**

    -   Moran's I is standardized to create a test statistic.

    -   The test statistic follows a normal distribution under the null hypothesis.

6.  **p-Value:**

    -   The p-value is calculated based on the test statistic and compared to a significance level (e.g., 0.05).

7.  **Decision:**

    -   If the p-value is less than the significance level, you reject the null hypothesis, suggesting the presence of spatial autocorrelation.

    -   If the p-value is greater than the significance level, you fail to reject the null hypothesis, suggesting a random spatial pattern.

Moran's test is commonly used in spatial analysis to identify spatial patterns in data, especially in fields such as geography, ecology, and economics. It helps researchers understand whether observed values exhibit clustering or dispersion in space.

```{r}
moran.test(hunan$GDPPC, 
           listw=rswm_q, 
           zero.policy = TRUE, 
           na.action=na.omit)
```

Given that the p-value is less than 0.05, there is sufficient evidence to reject the Null Hypothesis, and there is spatial autocorrelation in the dataset.

### Computing Monte Carlo Moran's I

Monte Carlo Moran's I is an extension of the traditional Moran's I test that involves using Monte Carlo simulation to assess the statistical significance of the observed Moran's I statistic. The traditional Moran's I test compares the observed Moran's I value to its expected value under the null hypothesis, but Monte Carlo Moran's I provides a more robust assessment by generating a distribution of Moran's I values under the null hypothesis through repeated random permutations of the data.

Here's how Monte Carlo Moran's I is typically conducted:

1.  **Calculate Observed Moran's I:**

    -   Compute the Moran's I statistic using the actual data.

2.  **Generate Random Permutations:**

    -   Randomly shuffle the values of the variable among spatial units to create a set of spatially randomized datasets. This preserves the overall distribution of the variable but disrupts any spatial patterns.

3.  **Calculate Moran's I for Randomized Datasets:**

    -   For each randomized dataset, compute the Moran's I statistic.

4.  **Compare Observed Moran's I to Randomized Distributions:**

    -   Compare the observed Moran's I statistic to the distribution of Moran's I values obtained from the randomized datasets.

5.  **Calculate p-Value:**

    -   The p-value is calculated based on the proportion of times the Moran's I value from the randomized datasets is greater than or equal to the observed Moran's I.

6.  **Decision:**

    -   If the observed Moran's I is significantly different from the distribution of Moran's I values from the randomized datasets, you may reject the null hypothesis of no spatial autocorrelation.

Monte Carlo Moran's I is particularly useful when dealing with small sample sizes and non-normal distributions, where the traditional asymptotic distribution of Moran's I may not be accurate. By simulating the null distribution, the Monte Carlo approach provides a more reliable assessment of statistical significance.

The code chunk below performs permutation test for Moran's I statistic by using [*moran.mc()*](https://r-spatial.github.io/spdep/reference/moran.mc.html) of **spdep**. A total of 1000 simulation will be performed.

```{r}
set.seed(1234)
bperm= moran.mc(hunan$GDPPC, 
                listw=rswm_q, 
                nsim=999, 
                zero.policy = TRUE, 
                na.action=na.omit)
bperm
```

-   The observed Moran's I value is **`0.30075`**. This value represents the degree of spatial autocorrelation in the observed data. Positive values suggest clustering, meaning that similar values tend to be close to each other in space.

-   The p-value is **`0.001`**, which is less than the conventional significance level of 0.05. This suggests that the observed Moran's I is statistically significant.

-   The alternative hypothesis being "greater" indicates that the spatial autocorrelation is in the positive direction (values are positively correlated in space).

In summary, the result suggests that there is significant positive spatial autocorrelation in the **`GDPPC`** variable in the **`hunan`** dataset using the specified spatial weights.

### Visualising Monte Carlo Moran's I

It is always a good practice for us the examine the simulated Moran's I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.

In the code chunk below [*hist()*](https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/hist) and [*abline()*](https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/abline) of R Graphics are used.

```{r}
mean(bperm$res[1:999])
```

```{r}
var(bperm$res[1:999])
```

```{r}
summary(bperm$res[1:999])
```

```{r}
hist(bperm$res, 
     freq=TRUE, 
     breaks=20, 
     xlab="Simulated Moran's I")
abline(v=0, 
       col="red") 
```

-   The histogram shows the distribution of simulated Moran's I values obtained through the Monte Carlo simulation (permutation test).

-   The vertical red line at x=0 indicates the observed Moran's I value. If this line falls far to the right or left of the distribution, it provides an indication of whether the observed Moran's I is statistically significant.

-   If the observed Moran's I falls into the extreme tail of the distribution, it suggests that the observed spatial autocorrelation is unlikely to have occurred by random chance.

Based on the histogram - it suggests a weak spatial autocorrelation.

```{r}
library(ggplot2)

# Assuming bperm$res is your vector of simulated Moran's I values
simulated_morans_i <- bperm$res

# Create a histogram using ggplot2
ggplot(data.frame(morans_i = simulated_morans_i), aes(x = morans_i)) +
  geom_histogram(binwidth = 0.01, fill = "lightblue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Simulated Moran's I",
       x = "Simulated Moran's I",
       y = "Frequency") +
  theme_minimal() +
  geom_vline(xintercept = 0, color = "red", linetype = "dashed", size = 1)

```

## **Global Spatial Autocorrelation: Geary's C**

Geary's C is a measure of global spatial autocorrelation that evaluates whether the values of a variable in a dataset are spatially clustered or dispersed. It is often used as an alternative to Moran's I. Geary's C is sensitive to both positive and negative spatial autocorrelation.

The formula for Geary's C is given by:

![](Geary%20test.png)

Geary's C values range from 0 to 2. A value close to 1 indicates spatial randomness, while values significantly different from 1 suggest spatial autocorrelation. If C \< 1, it suggests positive spatial autocorrelation (clustering), and if C \> 1, it suggests negative spatial autocorrelation (dispersion).

To perform a test of significance for Geary's C, one can compare the calculated statistic to its expected value under the null hypothesis (no spatial autocorrelation). A Monte Carlo simulation or permutation test is often used for this purpose.

```{r}
geary.test(hunan$GDPPC, listw=rswm_q)
```

-   **Geary C Statistic Standard Deviate:** The standard deviate is a measure of how many standard deviations the observed Geary's C statistic is from its expected value under the null hypothesis of no spatial autocorrelation. In your case, it's given as **`3.6108`**.

-   **p-Value:** The p-value associated with the observed Geary's C statistic is **`0.0001526`**. This is a very small p-value, indicating strong evidence against the null hypothesis. A small p-value typically suggests that the observed spatial pattern is unlikely to have occurred by random chance.

-   **Alternative Hypothesis:** The alternative hypothesis is stated as "Expectation greater than statistic," suggesting that the observed Geary's C is smaller than expected under the null hypothesis.

-   **Sample Estimates:**

    -   **Geary C Statistic:** The observed Geary's C statistic is **`0.6907223`**.

    -   **Expectation:** The expected value of Geary's C under the null hypothesis is **`1.0000000`**.

    -   **Variance:** The variance of the Geary's C statistic is **`0.0073364`**.

**Interpretation:**

-   The observed Geary's C statistic is **`0.6907223`**. This value is less than 1, suggesting positive spatial autocorrelation (clustering) in the **`GDPPC`** variable.

-   The small p-value (**`0.0001526`**) indicates that the observed spatial pattern is statistically significant, and you would reject the null hypothesis of no spatial autocorrelation.

-   The expectation under the null hypothesis is **`1.0000000`**, and the observed value is significantly smaller than expected, reinforcing the evidence of positive spatial autocorrelation.

In summary, the Geary's C test with randomization suggests that there is a significant positive spatial autocorrelation in the distribution of **`GDPPC`** in the **`hunan`** dataset based on the specified spatial weights **`rswm_q`**. The values are clustered in space more than would be expected by random chance.

#### Computing Monte Carlo Geary's C

```{r}
set.seed(1234)
bperm=geary.mc(hunan$GDPPC, 
               listw=rswm_q, 
               nsim=999)
bperm
```

-   **Number of Simulations:** The Monte Carlo simulation was conducted with 1000 simulations (**`number of simulations + 1: 1000`**).

-   **Observed Statistic:** The observed Geary's C statistic is given as **`0.69072`**.

-   **Observed Rank:** The observed rank is specified as **`1`**. This indicates that the observed Geary's C statistic falls in the top 1 (out of 1000) of the simulated values. A lower rank indicates stronger evidence against the null hypothesis.

-   **p-Value:** The p-value associated with the observed Geary's C is **`0.001`**.

-   **Alternative Hypothesis:** The alternative hypothesis is stated as "greater," suggesting that the observed Geary's C is greater than expected under the null hypothesis.

**Interpretation:**

-   The observed Geary's C statistic is **`0.69072`**, and it falls in the top 1 rank out of 1000 simulated values. This suggests that the observed spatial autocorrelation in the distribution of **`GDPPC`** is significant.

-   The small p-value (**`0.001`**) indicates strong evidence against the null hypothesis of no spatial autocorrelation. It suggests that the observed spatial pattern is unlikely to have occurred by random chance.

-   The alternative hypothesis being "greater" aligns with the interpretation that the observed spatial autocorrelation is greater than expected under the null hypothesis.

In summary, the Monte Carlo simulation of Geary's C provides support for the presence of significant positive spatial autocorrelation in the **`GDPPC`** variable in the **`hunan`** dataset based on the specified spatial weights **`rswm_q`**. The values are more clustered in space than would be expected by random chance.

### Visualising the Monte Carlo Geary's C

```{r}
mean(bperm$res[1:999])
```

```{r}
var(bperm$res[1:999])
```

```{r}
summary(bperm$res[1:999])
```

```{r}
hist(bperm$res, freq=TRUE, breaks=20, xlab="Simulated Geary c")
abline(v=1, col="red") 
```

## Spatial Correlogram

Spatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran's I or Geary's c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.

### **Compute Moran's I correlogram**

In the code chunk below, [*sp.correlogram()*](https://r-spatial.github.io/spdep/reference/sp.correlogram.html) of **spdep** package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran's I. The **plot()** of base Graph is then used to plot the output.

```{r}
MI_corr <- sp.correlogram(wm_q, 
                          hunan$GDPPC, 
                          order=6, 
                          method="I", 
                          style="W")
plot(MI_corr)
```

By plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.

```{r}
print(MI_corr)
```

-   **Lag Distance (88):** The numbers in parentheses represent the lag distance or distance between pairs of locations. Each row corresponds to a specific lag distance.

-   **Estimate:** The estimate column provides the observed Moran's I values at each lag distance. These values quantify the degree of spatial autocorrelation at different distances.

-   **Expectation:** The expectation column shows the expected value of Moran's I under the null hypothesis of spatial randomness. It represents what you would expect to observe if there were no spatial autocorrelation.

-   **Variance:** The variance column indicates the variance of Moran's I at each lag distance. It provides a measure of the variability in Moran's I.

-   **Standard Deviate:** The standard deviate is a standardized measure of how many standard deviations the observed Moran's I is from its expected value. It is calculated as (estimate−expectation)/variance(estimate−expectation)/variance​.

-   **Pr(I):** This column represents the p-value associated with Moran's I at each lag distance. It indicates whether the observed spatial autocorrelation is statistically significant.

-   **Two-Sided Significance Codes:** The significance codes provide a quick reference to the level of significance. The codes include '***' (highly significant), '**' (significant), '*' (marginally significant), '.' (not significant at conventional levels).

**Interpretation:**

-   At a lag distance of 88, the observed Moran's I is compared to its expected value under the null hypothesis.

-   The "Pr(I)" column provides p-values, and the significance codes indicate whether the observed Moran's I is statistically significant at different lag distances.

-   For example, at lag distance 1, the Moran's I value is 0.3007500 with a very small p-value (2.189e-06), indicating highly significant positive spatial autocorrelation.

In summary, the spatial correlogram suggests that there is significant positive spatial autocorrelation in the **`GDPPC`** variable at various lag distances, based on Moran's I. The values are clustered or dispersed in space more than would be expected by random chance.

### **Compute Geary's C correlogram and plot**

In the code chunk below, *sp.correlogram()* of **spdep** package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary's C. The **plot()** of base Graph is then used to plot the output.

```{r}
GC_corr <- sp.correlogram(wm_q, 
                          hunan$GDPPC, 
                          order=6, 
                          method="C", 
                          style="W")
plot(GC_corr)
```

```{r}
print(GC_corr)
```

**Interpretation:**

-   At a lag distance of 88, the observed Geary's C is compared to its expected value under the null hypothesis.

-   The "Pr(I)" column provides p-values, and the significance codes indicate whether the observed Geary's C is statistically significant at different lag distances.

-   For example, at lag distance 1, the Geary's C value is 0.6907223 with a very small p-value (0.0003052), indicating highly significant negative spatial autocorrelation.

In summary, the spatial correlogram with Geary's C suggests that there is significant spatial autocorrelation in the **`GDPPC`** variable at various lag distances. The values exhibit clustering or dispersion in space more than would be expected by random chance.

## Cluster and Outlier Analysis

Local Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance if we are studying cancer rates among census tracts in a given city local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.

In this section, we will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran'I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.

### Compute Local Moran's I

To compute local Moran's I, the [*localmoran()*](https://r-spatial.github.io/spdep/reference/localmoran.html) function of **spdep** will be used. It computes *Ii* values, given a set of *zi* values and a listw object providing neighbour weighting information for the polygon associated with the zi values.

The code chunks below are used to compute local Moran's I of *GDPPC2012* at the county level.

```{r}
fips <- order(hunan$County)
localMI <- localmoran(hunan$GDPPC, rswm_q)
head(localMI)
```

*localmoran()* function returns a matrix of values whose columns are:

-   Ii: the local Moran's I statistics

-   E.Ii: the expectation of local moran statistic under the randomisation hypothesis

-   Var.Ii: the variance of local moran statistic under the randomisation hypothesis

-   Z.Ii:the standard deviate of local moran statistic

-   Pr(): the p-value of local moran statistic

The code chunk below list the content of the local Moran matrix derived by using [*printCoefmat()*](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/printCoefmat).

```{r}
printCoefmat(data.frame(
  localMI[fips,], 
  row.names=hunan$County[fips]),
  check.names=FALSE)
```

### Mapping the Local Moran's I

Before mapping the local Moran's I map, it is wise to append the local Moran's I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called *hunan.localMI*.

```{r}
hunan.localMI <- cbind(hunan,localMI) %>%
  rename(Pr.Ii = Pr.z....E.Ii..)
```

#### Mapping local Moran's I values

Using choropleth mapping functions of **tmap** package, we can plot the local Moran's I values by using the code chinks below.

```{r}
tm_shape(hunan.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty",
          palette = "RdBu",
          title = "local moran statistics") +
  tm_borders(alpha = 0.5)
```

#### Mapping local Moran's I p-values

The choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.

The code chunks below produce a choropleth map of Moran's I p-values by using functions of **tmap** package.

```{r}
tm_shape(hunan.localMI) +
  tm_fill(col = "Pr.Ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5)
```

#### Mapping both I values and P values

```{r}
localMI.map <- tm_shape(hunan.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty", 
          title = "local moran statistics") +
  tm_borders(alpha = 0.5)

pvalue.map <- tm_shape(hunan.localMI) +
  tm_fill(col = "Pr.Ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5)

tmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)
```

## Creating a LISA Cluster Map

The LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.

Here's how a LISA cluster map typically works:

1.  **Local Moran's I Calculation:**

    -   Local Moran's I is calculated for each location in the dataset. Local Moran's I measures the degree of spatial autocorrelation for a specific location and its neighboring locations.

    -   The formula for Local Moran's I is similar to the global Moran's I but focuses on local relationships.

2.  **Categorization of Local Moran's I Values:**

    -   Each location is then categorized into one of four groups based on the sign (positive or negative) and significance of its Local Moran's I value:

        -   High-High (HH): High values surrounded by high values.

        -   Low-Low (LL): Low values surrounded by low values.

        -   High-Low (HL): High values surrounded by low values.

        -   Low-High (LH): Low values surrounded by high values.

3.  **LISA Cluster Map:**

    -   The LISA cluster map is a spatial map where each location is color-coded or marked according to its LISA category.

    -   This map provides a visual representation of spatial patterns in the dataset, showing where clusters of similar or dissimilar values are located.

4.  **Interpretation:**

    -   The clusters on the LISA map help identify areas of spatial concentration or dispersion of values.

    -   High-High and Low-Low clusters represent areas with significant positive spatial autocorrelation (clustering of similar values) or negative spatial autocorrelation (clustering of dissimilar values), respectively.

    -   High-Low and Low-High clusters represent areas with significant spatial outliers, where high values are surrounded by low values or vice versa.

### Plotting Moran Scatterplot

The Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.

The code chunk below plots the Moran scatterplot of GDPPC 2012 by using [*moran.plot()*](https://r-spatial.github.io/spdep/reference/moran.plot.html) of **spdep**.

```{r}
nci <- moran.plot(hunan$GDPPC, rswm_q,
                  labels=as.character(hunan$County), 
                  xlab="GDPPC 2012", 
                  ylab="Spatially Lag GDPPC 2012")
```

Notice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.

### Plotting Moran scatterplot with standardised variable

First we will use [*scale()*](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/scale) to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\

```{r}
hunan$Z.GDPPC <- scale(hunan$GDPPC) %>% 
  as.vector 
```

The [*as.vector()*](https://www.rdocumentation.org/packages/pbdDMAT/versions/0.5-1/topics/as.vector) added to the end is to make sure that the data type we get out of this is a vector, that map neatly into our dataframe.

Now, we are ready to plot the Moran scatterplot again by using the code chunk below.

```{r}
nci2 <- moran.plot(hunan$Z.GDPPC, rswm_q,
                   labels=as.character(hunan$County),
                   xlab="z-GDPPC 2012", 
                   ylab="Spatially Lag z-GDPPC 2012")
```

### Preparing LISA Map Classes

The code chunks below show the steps to prepare a LISA cluster map.

```{r}
quadrant <- vector(mode="numeric",length=nrow(localMI))
```

Next, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.

```{r}
hunan$lag_GDPPC <- lag.listw(rswm_q, hunan$GDPPC)
DV <- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     
```

This is follow by centering the local Moran's around the mean.

```{r}
LM_I <- localMI[,1] - mean(localMI[,1])    
```

Next, we will set a statistical significance level for the local Moran.

```{r}
signif <- 0.05       

```

These four command lines define the (1) low-low, (2) high-low (3), low-high and (4) high-high categories.

```{r}
quadrant[DV <0 & LM_I>0] <- 1 #low-low
quadrant[DV >0 & LM_I<0] <- 2 #high-low
quadrant[DV <0 & LM_I<0] <- 3 #low-high 
quadrant[DV >0 & LM_I>0] <- 4 #high-high
```

Lastly, places non-significant Moran in the category 0.

```{r}
quadrant[localMI[,5]>signif] <- 0
```

In fact, we can combined all the steps into one single code chunk as shown below:

```{r}
quadrant <- vector(mode="numeric",length=nrow(localMI))
hunan$lag_GDPPC <- lag.listw(rswm_q, hunan$GDPPC)
DV <- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     
LM_I <- localMI[,1]   
signif <- 0.05       
quadrant[DV <0 & LM_I>0] <- 1
quadrant[DV >0 & LM_I<0] <- 2
quadrant[DV <0 & LM_I<0] <- 3  
quadrant[DV >0 & LM_I>0] <- 4    
quadrant[localMI[,5]>signif] <- 0
```

### Plotting LISA Map

Now, we can build the LISA map by using the code chunks below.

```{r}
hunan.localMI$quadrant <- quadrant
colors <- c("#ffffff", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c")
clusters <- c("insignificant", "low-low", "low-high", "high-low", "high-high")

tm_shape(hunan.localMI) +
  tm_fill(col = "quadrant", 
          style = "cat", 
          palette = colors[c(sort(unique(quadrant)))+1], 
          labels = clusters[c(sort(unique(quadrant)))+1],
          popup.vars = c("")) +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_borders(alpha=0.5)
```

For effective interpretation, it is better to plot both the local Moran's I values map and its corresponding p-values map next to each other.

The code chunk below will be used to create such visualisation.

```{r}
gdppc <- qtm(hunan, "GDPPC")

hunan.localMI$quadrant <- quadrant
colors <- c("#ffffff", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c")
clusters <- c("insignificant", "low-low", "low-high", "high-low", "high-high")

LISAmap <- tm_shape(hunan.localMI) +
  tm_fill(col = "quadrant", 
          style = "cat", 
          palette = colors[c(sort(unique(quadrant)))+1], 
          labels = clusters[c(sort(unique(quadrant)))+1],
          popup.vars = c("")) +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_text("County", size = 0.3, col = "black") +
  tm_borders(alpha=0.5)

tmap_arrange(gdppc, LISAmap, 
             asp=1, ncol=2)
```

We can also include the local Moran's I map and p-value map as shown below for easy comparison.

![](Moran%20p%20and%20i%20value)

**Interpretation**

-   Changsha is one of the county in Hunan with the highest GDPPC and the region surrounding Changsha are closely correlated - high-high group. Also given that Changsha is Hunan's capital.

-   PingJiang and Tao Jiang are areas surrounding the high regions but are low instead. This could be due to their terrain factor which limits development in these areas - Both are in the more hilly and mountainous areas.

## **Hot Spot and Cold Spot Area Analysis**

Beside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.

The term 'hot spot' has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).

### **Getis and Ord's G-Statistics**

An alternative spatial statistics to detect spatial anomalies is the Getis and Ord's G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.

The analysis consists of three steps:

-   Deriving spatial weight matrix

-   Computing Gi statistics

-   Mapping Gi statistics

**Interpretation of Getis-Ord G Statistics:**

-   **High Positive Z-Scores:** Locations with high positive z-scores are hotspots. This means that the values at these locations are higher than expected and are surrounded by other locations with high values.

-   **Low Negative Z-Scores:** Locations with low negative z-scores are cold spots. This means that the values at these locations are lower than expected and are surrounded by other locations with low values.

-   **Not Statistically Significant:** If the z-score is not statistically significant, it suggests that the observed clustering is likely due to random chance, and there is no clear evidence of hotspots or cold spots.

### Deriving spatial weight matrix

These were derived in the earlier sections.

First, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.

There are two type of distance-based proximity matrix, they are:

-   fixed distance weight matrix; and

```{r}
wm62_lw <- nb2listw(wm_d62, style = 'B')
summary(wm62_lw)
```

-   adaptive distance weight matrix

```{r}
knn <- knn2nb(knearneigh(coords, k=8))
knn
```

```{r}
knn_lw <- nb2listw(knn, style = 'B')
summary(knn_lw)
```

### Computing GI statistics

#### Using Fixed distance

```{r}
fips <- order(hunan$County)
gi.fixed <- localG(hunan$GDPPC, wm62_lw)
gi.fixed
```

The output of localG() is a vector of G or Gstar values, with attributes "gstari" set to TRUE or FALSE, "call" set to the function call, and class "localG".

The Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.

Next, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.

```{r}
hunan.gi <- cbind(hunan, as.matrix(gi.fixed)) %>%
  rename(gstat_fixed = as.matrix.gi.fixed.)
```

#### Mapping Gi values with fixed distance weights

The code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.

```{r}
gdppc <- qtm(hunan, "GDPPC")

Gimap <-tm_shape(hunan.gi) +
  tm_fill(col = "gstat_fixed", 
          style = "pretty",
          palette="-RdBu",
          title = "local Gi") +
  tm_borders(alpha = 0.5) +
  tm_text("County", size = 0.2, col = "black")

tmap_arrange(gdppc, Gimap, asp=1, ncol=2)
```

Interpretation - Hotspots are towards the NE region of Hunan, the western regions are generally less developed.

#### Using Adaptive Distance

The code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e *knb_lw*).

```{r}
fips <- order(hunan$County)
gi.adaptive <- localG(hunan$GDPPC, knn_lw)
hunan.gi <- cbind(hunan, as.matrix(gi.adaptive)) %>%
  rename(gstat_adaptive = as.matrix.gi.adaptive.)
```

#### Mapping Gi values with adaptive distance

It is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of **tmap** package will be used to map the Gi values.

The code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.

```{r}
gdppc<- qtm(hunan, "GDPPC")

Gimap <- tm_shape(hunan.gi) + 
  tm_fill(col = "gstat_adaptive", 
          style = "pretty", 
          palette="-RdBu", 
          title = "local Gi") + 
  tm_borders(alpha = 0.5) +
  tm_text("County", size = 0.2, col = "black")

tmap_arrange(gdppc, 
             Gimap, 
             asp=1, 
             ncol=2)
```
